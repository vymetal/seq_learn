{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f58496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqdata import getset\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pylab as plt\n",
    "import numpy\n",
    "from numpy import argmax, vstack\n",
    "import random\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim, nn, utils\n",
    "#from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.utils.data import random_split\n",
    "#from torch.nn import Linear,ReLU, LeakyReLU,ELU,Softmax,Module,CrossEntropyLoss\n",
    "#from torch.optim import SGD,Adam\n",
    "#from torch.nn.init import kaiming_uniform_,xavier_uniform_, uniform_, ones_, zeros_, eye_\n",
    "#from torch.nn.parameter import Parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87685249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xsample(x0,size):\n",
    "    res=[]\n",
    "    while size-len(res)>=len(x0):\n",
    "        res.extend(x0)\n",
    "    res.extend(random.sample(x0,size-len(res)))\n",
    "    return res\n",
    "\n",
    "def prepare_data(cats, cat_size=1000, batch_size=10):\n",
    "    X,y=getset(7)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1234567, shuffle=True)\n",
    "    \n",
    "    u,c=numpy.unique(y_train, return_counts=True)\n",
    "    print(f\"Counts for minimal group: {numpy.min(c)}\")\n",
    "    print(f\"Counts for maximal group: {numpy.max(c)}\")\n",
    "    \n",
    "    joined=[(x,lab) for x,lab in zip(X_train,y_train) if lab in cats]\n",
    "    X_train=[x for x,lab in joined]\n",
    "    y_train=[lab for x,lab in joined]\n",
    "    \n",
    "    joined=[(x,lab) for x,lab in zip(X_test,y_test) if lab in cats]\n",
    "    X_test=[x for x,lab in joined]\n",
    "    y_test=[lab for x,lab in joined]\n",
    "    \n",
    "    \n",
    "    test=[[numpy.array(x,dtype=numpy.float32),y] for x,y in zip(X_test,numpy.array(y_test,dtype=numpy.int64))]    \n",
    "    if cat_size>0:\n",
    "        train=[]    \n",
    "        #resample X_train and y_train\n",
    "        random.seed(None) #1234567)\n",
    "        new_X,new_y=[],[]\n",
    "        for cat in cats:\n",
    "            cat_xy=[(x,lab) for x,lab in zip(X_train,y_train) if lab==cat]\n",
    "            qs=xsample(cat_xy, cat_size)\n",
    "            #qs=[random.choice(cat_xy) for ii in range(cat_size)]\n",
    "            qs=[[numpy.array(x,dtype=numpy.float32),numpy.array(lab,dtype=numpy.int64)] for x,lab in qs]\n",
    "            train.extend(qs)\n",
    "    else:\n",
    "        train=[[numpy.array(x, dtype=numpy.float32),y] for x,y in zip(X_train,numpy.array(y_train,dtype=numpy.int64))]\n",
    "   \n",
    "    print(f'train data: {len(train)}, test data: {len(test)}, categories: {len(cats)}' )\n",
    "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1000000, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482c07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        N=n_inputs\n",
    "        \n",
    "        self.hidden1 = nn.Linear(n_inputs, N)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "\n",
    "        self.hidden_last = nn.Linear(N, 20)   \n",
    "        self.act_last = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        #last\n",
    "        X = self.hidden_last(X)\n",
    "        X = self.act_last(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4057a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLModel(pl.LightningModule):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        yhat = self.model(x)\n",
    "        loss = nn.CrossEntropyLoss()(yhat, y)\n",
    "        \n",
    "        ind = numpy.argmax(yhat.detach().numpy(),axis=1)\n",
    "        accuracy=accuracy_score(ind,y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        #print(training_step_outputs)\n",
    "        self.log(\"mean_train_loss\", torch.mean(torch.stack([x[\"loss\"] for x in training_step_outputs])))\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx = 0):\n",
    "        return self(batch)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x)\n",
    "        #print(x.shape)\n",
    "        return self.model.forward(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        #optimizer = optim.SGD(self.parameters(), lr=1e-2, momentum=0.9)\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4794e95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=PLModel(MLP(161))\n",
    "train,test=prepare_data(range(20),1000,1000000)\n",
    "#model.dropout=nn.Dropout(0.5)\n",
    "for cyc in range(1):\n",
    "    trainer = pl.Trainer(limit_train_batches=100, max_epochs=50,log_every_n_steps=1)\n",
    "    trainer.fit(model=model, train_dataloaders=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx=[x for x,_ in test][0]\n",
    "ty=[cat for _,cat in test][0].detach().numpy()\n",
    "print(ty)\n",
    "\n",
    "predictions = trainer.predict(model, tx)\n",
    "ind = [numpy.argmax(p) for p in predictions]\n",
    "accuracy=accuracy_score(ind,ty)\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "\n",
    "##truth=torch.stack([cat for _,cat in test]).detach().numpy()\n",
    "##print(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca31c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093e20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
