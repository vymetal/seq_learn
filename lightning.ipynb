{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f58496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqdata import getset\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pylab as plt\n",
    "import numpy\n",
    "from numpy import argmax, vstack\n",
    "import random\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim, nn, utils\n",
    "#from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.utils.data import random_split\n",
    "#from torch.nn import Linear,ReLU, LeakyReLU,ELU,Softmax,Module,CrossEntropyLoss\n",
    "#from torch.optim import SGD,Adam\n",
    "#from torch.nn.init import kaiming_uniform_,xavier_uniform_, uniform_, ones_, zeros_, eye_\n",
    "#from torch.nn.parameter import Parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87685249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xsample(x0,size):\n",
    "    res=[]\n",
    "    while size-len(res)>=len(x0):\n",
    "        res.extend(x0)\n",
    "    res.extend(random.sample(x0,size-len(res)))\n",
    "    return res\n",
    "\n",
    "def prepare_data(cats, cat_size=1000, batch_size=10):\n",
    "    X,y=getset(7)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1234567, shuffle=True)\n",
    "    \n",
    "    u,c=numpy.unique(y_train, return_counts=True)\n",
    "    print(f\"Counts for minimal group: {numpy.min(c)}\")\n",
    "    print(f\"Counts for maximal group: {numpy.max(c)}\")\n",
    "    \n",
    "    joined=[(x,lab) for x,lab in zip(X_train,y_train) if lab in cats]\n",
    "    X_train=[x for x,lab in joined]\n",
    "    y_train=[lab for x,lab in joined]\n",
    "    \n",
    "    \n",
    "    joined=[(x,lab) for x,lab in zip(X_test,y_test) if lab in cats]\n",
    "    X_test=[x for x,lab in joined]\n",
    "    y_test=[lab for x,lab in joined]\n",
    "    \n",
    "    \n",
    "    test=[[numpy.array(x,dtype=numpy.float32),y] for x,y in zip(X_test,numpy.array(y_test,dtype=numpy.int64))]    \n",
    "    if cat_size>0:\n",
    "        train=[]    \n",
    "        #resample X_train and y_train\n",
    "        random.seed(None) #1234567)\n",
    "        new_X,new_y=[],[]\n",
    "        for cat in cats:\n",
    "            cat_xy=[(x,lab) for x,lab in zip(X_train,y_train) if lab==cat]\n",
    "            qs=xsample(cat_xy, cat_size)\n",
    "            #qs=[random.choice(cat_xy) for ii in range(cat_size)]\n",
    "            qs=[[numpy.array(x,dtype=numpy.float32),numpy.array(lab,dtype=numpy.int64)] for x,lab in qs]\n",
    "            train.extend(qs)\n",
    "    else:\n",
    "        train=[[numpy.array(x, dtype=numpy.float32),y] for x,y in zip(X_train,numpy.array(y_train,dtype=numpy.int64))]\n",
    "   \n",
    "    print(f'train data: {len(train)}, test data: {len(test)}, categories: {len(cats)}' )\n",
    "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1000000, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482c07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        N=161*10\n",
    "        \n",
    "        self.hidden1 = nn.Linear(n_inputs, N)\n",
    "        nn.init.kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "\n",
    "  \n",
    "   #     self.hidden2 = nn.Linear(N, N)\n",
    "   #     nn.init.kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "   #     self.act2 = nn.LeakyReLU()\n",
    " \n",
    "   #     self.hidden3 = nn.Linear(N, N)\n",
    "   #     nn.init.kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
    "   #     self.act3 = nn.LeakyReLU()\n",
    "        \n",
    "   #     self.hidden4 = nn.Linear(N, N)\n",
    "   #     nn.init.kaiming_uniform_(self.hidden4.weight, nonlinearity='relu')\n",
    "   #     self.act4 = nn.LeakyReLU()\n",
    "        \n",
    "   #     self.hidden5 = nn.Linear(N, N)\n",
    "   #     nn.init.kaiming_uniform_(self.hidden5.weight, nonlinearity='relu')\n",
    "   #     self.act5 = nn.LeakyReLU()\n",
    " \n",
    "\n",
    "        self.hidden_last = nn.Linear(N, 20)   \n",
    "        nn.init.kaiming_uniform_(self.hidden_last.weight)        \n",
    "        #xavier_uniform_(self.hidden_last.weight)\n",
    "        #uniform_(self.hidden_last.weight)\n",
    "        #ones_(self.hidden_last.weight)\n",
    "        self.act_last = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.dropout(X)    #\n",
    "#        X = self.hidden2(X)\n",
    "#        X = self.act2(X)\n",
    "#        X = self.dropout(X)    #\n",
    "#        X = self.hidden3(X)\n",
    "#        X = self.act3(X)        \n",
    "        \n",
    "#        X = self.dropout(X)    #\n",
    "#        X = self.hidden4(X)\n",
    "#        X = self.act4(X)       \n",
    "        \n",
    "#        X = self.dropout(X)    #\n",
    "#        X = self.hidden5(X)\n",
    "#        X = self.act5(X)       \n",
    "        \n",
    "        #last\n",
    "        X = self.hidden_last(X)\n",
    "        X = self.act_last(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4057a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLModel(pl.LightningModule):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        yhat = self.model(x)\n",
    "        loss = nn.CrossEntropyLoss()(yhat, y)\n",
    "        \n",
    "        ind = numpy.argmax(yhat.detach().numpy(),axis=1)\n",
    "        accuracy=accuracy_score(ind,y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        #print(training_step_outputs)\n",
    "        self.log(\"mean_train_loss\", torch.mean(torch.stack([x[\"loss\"] for x in training_step_outputs])))\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx = 0):\n",
    "        return self(batch)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x)\n",
    "        #print(x.shape)\n",
    "        return self.model.forward(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        #optimizer = optim.SGD(self.parameters(), lr=1e-2, momentum=0.9)\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4794e95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07929682731628418,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4255432a994c1bb36e7b084c038eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10369563102722168,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1784713b7e46d18f0e7872ad3275d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.09616303443908691,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c316fccb1749aaa5c1a8caf836e6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10315251350402832,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43f882827ac4bbdacf4cabf44a1c46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07185029983520508,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c215a46c3b64322af1510deb37a2bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0881659984588623,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5259b81a2f1f48aa85c3ccf9d27c2b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.09325623512268066,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6681309acc443ba9b25e28194a4aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.17303061485290527,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f259a999cf6f4f04b04a9a766c9801bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10297441482543945,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc0bc108efd4bf7b229042c2983be36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.08816695213317871,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c18a68d516942e9bd63889e0224247e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.09799075126647949,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bf910ce7a7441d97d21daa5468cc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.12950682640075684,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c461418ef66944f98f33e2346027edb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07842016220092773,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdb26df9d5e401691d6307c13248697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10379838943481445,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e012a7c97ee42b6968578931c58bb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.06981301307678223,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1f3564ab2a4105a70fb32f6a93be59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10925674438476562,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52710d4d2cab4befa06b1894da121e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.12622404098510742,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcccc259a03745a7bb261709126383df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10243868827819824,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78336063176b4e2aadf47bdc06ce4e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.06859803199768066,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64568440bd284f889a52280e24077f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 293 K \n",
      "-------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.172     Total estimated model params size (MB)\n",
      "/home/jorky/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 3 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.06446504592895508,
       "initial": 0,
       "n": 0,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b630245b9614f3998ff7c95f284a101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "#model=PLModel(MLP(161))\n",
    "#train,test=prepare_data(range(20),1000,1000000)\n",
    "#model.dropout=nn.Dropout(0.5)\n",
    "for cyc in range(20):\n",
    "    trainer = pl.Trainer(limit_train_batches=100, max_epochs=50,log_every_n_steps=1)\n",
    "    trainer.fit(model=model, train_dataloaders=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b18f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2 11 ... 16  3 16]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07820248603820801,
       "initial": 1,
       "n": 1,
       "ncols": 196,
       "nrows": 59,
       "postfix": null,
       "prefix": "Predicting",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2396dc1bd454bfb98a742a312d5cd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.09013881377321074\n"
     ]
    }
   ],
   "source": [
    "tx=[x for x,_ in test][0]\n",
    "ty=[cat for _,cat in test][0].detach().numpy()\n",
    "print(ty)\n",
    "\n",
    "predictions = trainer.predict(model, tx)\n",
    "ind = [numpy.argmax(p) for p in predictions]\n",
    "accuracy=accuracy_score(ind,ty)\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "\n",
    "##truth=torch.stack([cat for _,cat in test]).detach().numpy()\n",
    "##print(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca31c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093e20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
